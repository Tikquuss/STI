{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1)en_details.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rf9QmETvIn_8"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"LHIgXfRrJEZa"},"source":["# ```stable_baselines``` ne supporte pas ```tf>2.x```"]},{"cell_type":"code","metadata":{"id":"rEwAkrAhIoml"},"source":["try:\n","    %tensorflow_version 1.x\n","    #%matplotlib inline\n","except Exception:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oq9vu6u7JVhY"},"source":["## Les imports necessaires"]},{"cell_type":"code","metadata":{"id":"Ssrs0wO-JUum"},"source":["import random\n","import json\n","import itertools\n","import datetime as dt\n","from IPython.display import clear_output\n","\n","import pandas as pd\n","import numpy as np\n","\n","# ! pip install stable-baselines\n","import gym\n","from gym import spaces\n","\n","from stable_baselines.common.policies import MlpPolicy\n","from stable_baselines.common.vec_env import DummyVecEnv\n","from stable_baselines import PPO2\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5NqvgP-mIgIg"},"source":["# Tuteur"]},{"cell_type":"code","metadata":{"id":"uo9DDuwZJQzX"},"source":["add = lambda x, y : x+y\r\n","mult = lambda x, y : x*y\r\n","subt = lambda x, y : x-y\r\n","eps = 1e-12 # avoid division by zero\r\n","div = lambda x, y : x/(y+eps)\r\n","\r\n","mathematical_operators = {\"+\" : add, \"*\" : mult, \"-\" : subt, \"/\" : div}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hecbn5nGIidk"},"source":["def get_dataset(low, high, n, operator = \"+\", n_choices = 2) :\r\n","    assert operator in mathematical_operators.keys()\r\n","    assert high - low + 1 > 2*n\r\n","    assert n_choices >= 2\r\n","    op = mathematical_operators[operator]\r\n","    ds = random.sample(range(low, high), 2*n)\r\n","    ds = zip(ds[:n], ds[n:])\r\n","\r\n","    qcms = []\r\n","\r\n","    for x in ds :\r\n","        r = op(x[0], x[1])\r\n","        #choices = [r, r + random.randint(-high, high)] + [r + random.randint(-high, high) for _ in range(n_choices-1)]\r\n","        choices = [r] + [r + random.randint(-high, high) for _ in range(n_choices-1)]\r\n","        random.shuffle(choices)\r\n","        qcms.append({\r\n","            \"question\" : tuple(list(x) + [operator]), # str(x[0])+\"+\"+str(x[1]) \r\n","            \"choices\" : choices, \r\n","            \"answer\" : r\r\n","        })\r\n","    return qcms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-FYIKzbZLrqf"},"source":["get_dataset(low = 1, high = 100, n = 5, n_choices = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9Uj8AslLvsz"},"source":["get_dataset(low = 1, high = 100, n = 5, n_choices = 3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZ4dYEVk3pIR"},"source":["# Apprénant"]},{"cell_type":"code","metadata":{"id":"q01CrY27Jfyo"},"source":["class Student(gym.Env):\n","    metadata = {'render.modes': ['human']}\n","\n","    def __init__(self, qcms, low, high, n_choices = 2, loss_threshold = 2):\n","        super(Student, self).__init__()\n","        assert n_choices >= 2\n","\n","        self.qcms = qcms\n","        self.low = low\n","        self.high = high\n","        self.n_choices = n_choices\n","        self.loss_threshold = loss_threshold\n","\n","        self.score = 0\n","        self.current_step = 0\n","\n","        self.batching = False\n","        \n","        # Actions of the format : 0 or n_choices-1\n","        self.action_space = spaces.Discrete(n_choices)\n","\n","        # Un qcm\n","        self.dic_spaces = {\n","          'question' : spaces.Box(low = np.array([self.low]*n_choices), high = np.array([self.high]*n_choices), dtype = np.int8),\n","          'choices' : self.action_space,\n","          'answer' : spaces.Box(low = self.low, high = self.high + self.high, shape = (1,), dtype = np.int8)\n","        }\n","\n","        self.observation_space = spaces.Dict(self.dic_spaces)\n","        #self.observation_space = spaces.Box(low = np.array([self.low]*n_choices), high = np.array([self.high]**n_choices), dtype = np.int8)\n","        \n","    def prepare_dataset(self, batch_size = 20) :\n","        ds = []\n","        n_samples = len(self.qcms) \n","        i = 0\n","        while n_samples > i :\n","            i += batch_size\n","            x = qcms[i-batch_size:i]\n","            ds.append({\n","                #\"question\" : [x_i[\"question\"] for x_i in x],  \n","                \"question\" : [x_i[\"question\"][:-1] for x_i in x], # ignore opérator\n","                \"choices\" : [x_i[\"choices\"] for x_i in x], \n","                \"answer\" : [x_i[\"answer\"] for x_i in x]\n","            })\n","\n","        self.ds = ds\n","        self.batching = True\n","        self.batch_size = batch_size\n","        self.action_space = spaces.Box(low = 0, high = self.n_choices-1, shape = (batch_size,), dtype = np.int8)\n","        self.dic_spaces = {\n","          'question' : spaces.Box(low = self.low, high = self.high, shape = (batch_size, self.n_choices), dtype = np.int8),\n","          'choices' : self.action_space,\n","          'answer' : spaces.Box(low = self.low, high = self.high + self.high, shape = (batch_size, 1,), dtype = np.int8)\n","        }\n","        self.observation_space = spaces.Dict(self.dic_spaces)\n","\n","    def _next_observation(self):\n","        if self.batching :\n","            return self.ds[self.current_step]\n","        else :\n","            return self.qcms[self.current_step]\n","\n","    def take_action(self, state, action) :\n","        return [x_i[y_i] for x_i, y_i in zip(state['choices'], action)]\n","        \n","    def _take_action(self, qcm, action):\n","        return qcm['choices'][action]\n","\n","    def policy(self, state, Q) :\n","        y_pred = Q(torch.Tensor(state[\"question\"])).reshape((self.batch_size,))\n","        action = []\n","        choices_tmp = np.arange(self.n_choices)\n","        for x_i, y_i in zip(state[\"choices\"], y_pred.detach().numpy()) :\n","            try :\n","              a = x_i.index(y_i)\n","            except ValueError:\n","              idx = np.argmin(np.absolute(y_i - np.array(x_i)))\n","              if abs(x_i[idx] - y_i) <= self.loss_threshold :\n","                  a = idx\n","              else : \n","                  a = np.random.choice(choices_tmp)\n","            action.append(a)\n","        return action, y_pred \n","\n","    def step(self, action):\n","        # Execute one time step within the environment\n","        self.current_step += 1\n","        \n","        if not self.batching :\n","            if self.current_step > len(self.qcms) - 1 :\n","                self.current_step = 0\n","            qcm = self._next_observation()\n","            answer = self._take_action(qcm, action)\n","            reward = 1 if answer == qcm[\"answer\"] else -0.25\n","            self.score += reward\n","            done = self.score / len(self.qcms) >= 0.5\n","            return qcm, reward, done, {}\n","\n","        else :\n","            done = False\n","            if self.current_step > len(self.ds) - 1 :\n","                self.current_step = 0\n","                #done = True\n","            qcm = self._next_observation()\n","            answer = self.take_action(qcm, action)\n","            reward = (np.array(answer) == np.array(qcm[\"answer\"])).astype(np.float)\n","            reward = [-0.25 if r == 0 else r for r in reward]\n","            reward = sum(reward) / self.batch_size \n","            self.score += reward\n","            done = self.score >= 0.5\n","            return qcm, reward, done, {}\n","\n","    def reset(self):\n","        # Reset the state of the environment to an initial state\n","        self.score = 0\n","        # Set the current step to a random point within the data frame\n","        if self.batching :\n","            self.current_step = random.randint(0, len(self.ds) - 1)\n","        else :\n","            self.current_step = random.randint(0, len(self.qcms) - 1)\n","\n","        return self._next_observation()\n","\n","    def render(self, mode='human', close=False):\n","        # Render the environment to the screen\n","        m = len(self.qcms) if not self.batching else len(self.ds)\n","        print(f'Score : {self.score} note {self.score / m}')\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B76UlR94NBeQ"},"source":["low = 1\r\n","high = 100\r\n","n = 10\r\n","qcms = get_dataset(low = low, high = high, n = n)\r\n","env = Student(qcms, low, high)\r\n","\r\n","env.render()\r\n","\r\n","print(\"Action Space {}\".format(env.action_space))\r\n","print(\"State Space {}\".format(env.observation_space))\r\n","\r\n","env.prepare_dataset(batch_size = 32)\r\n","print(\"Action Space {}\".format(env.action_space))\r\n","print(\"State Space {}\".format(env.observation_space))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPU00M54YFMt"},"source":["low = 1\r\n","high = 100\r\n","n = 10\r\n","n_choices = 3\r\n","qcms = get_dataset(low = low, high = high, n = n, n_choices = n_choices)\r\n","env = Student(qcms, low, high, n_choices = n_choices)\r\n","\r\n","env.render()\r\n","\r\n","print(\"Action Space {}\".format(env.action_space))\r\n","print(\"State Space {}\".format(env.observation_space))\r\n","\r\n","env.prepare_dataset(batch_size = 32)\r\n","print(\"Action Space {}\".format(env.action_space))\r\n","print(\"State Space {}\".format(env.observation_space))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bb2S28JENPEr"},"source":["# Entrainement"]},{"cell_type":"code","metadata":{"id":"i6FTBJKplmJx"},"source":["def train(env, n_episodes, Q, log_interval = 1) :\r\n","    %%time\r\n","    \"\"\"Training the agent\"\"\"\r\n","  \r\n","    # Hyperparameters\r\n","    #alpha = 0.1\r\n","    #gamma = 0.6\r\n","    epsilon = 0.1\r\n","\r\n","    # For plotting metrics\r\n","    #all_epochs = []\r\n","    #all_penalties = []\r\n","\r\n","    all_reward = {}\r\n","    all_loss = {}\r\n","\r\n","    env.reset()\r\n","    Q.train()\r\n","    for i in range(1, n_episodes + 1):\r\n","        \r\n","        all_reward[i] = [] \r\n","        all_loss[i] = []\r\n","        \r\n","        state = env.reset()\r\n","\r\n","        epochs, total_reward = 0, 0\r\n","        done = False\r\n","        \r\n","        while not done:\r\n","            Q.optimizer.zero_grad()\r\n","            #cond =  random.uniform(0, 1) < epsilon\r\n","            cond = False\r\n","            if cond :\r\n","                action = env.action_space.sample() # Explore action space\r\n","                y_pred = env.take_action(state, action)\r\n","                #print(action, state, y_pred)\r\n","            else:\r\n","                action, y_pred = env.policy(state, Q) # Exploit learned values\r\n","            \r\n","            next_state, reward, done, _ = env.step(action) \r\n","            y = torch.Tensor(state[\"answer\"])\r\n","            loss = Q.criterion(y, y_pred)\r\n","            print(\"reward\", reward, \"loss\", loss.item())\r\n","            all_loss[i].append(loss.item())\r\n","            all_reward[i].append(reward)\r\n","            total_reward += reward\r\n","\r\n","            loss.backward()\r\n","            Q.optimizer.step()\r\n","\r\n","            state = next_state\r\n","            epochs += 1\r\n","\r\n","            env.render()\r\n","            \r\n","        if i % log_interval == 0:\r\n","            clear_output(wait=True)\r\n","            print(f\"Episode: {i}, n_epochs : {epochs}, reward {total_reward}\")\r\n","\r\n","    print(\"Training finished.\\n\")\r\n","    return Q, all_loss, all_reward"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wb-lIgtalWYA"},"source":["def evaluate(env, n_episodes, Q = None, log_interval = 2):\r\n","    %%time\r\n","    \"\"\"Evaluate agent's performance after Q-learning\"\"\"\r\n","\r\n","    total_epochs, total_penalties = 0, 0\r\n","    frames_RL = {}\r\n","    \r\n","    if Q is not None :\r\n","        Q.eval()\r\n","\r\n","    for i in range(1, n_episodes + 1):\r\n","        state = env.reset()\r\n","        epochs, total_reward = 0, 0\r\n","        \r\n","        done = False\r\n","\r\n","        frames_RL[i] = []\r\n","        \r\n","        while not done:\r\n","            if Q is not None :\r\n","                action, y_pred = env.policy(state, Q)\r\n","                state, reward, done, _ = env.step(action)\r\n","                y = torch.Tensor(state[\"answer\"])\r\n","                loss = Q.criterion(y, y_pred)\r\n","            else :\r\n","                action = env.action_space.sample()\r\n","                state, reward, done, _ = env.step(action)\r\n","\r\n","            frames_RL[i].append({\r\n","                'frame': env.render(mode='ansi'),\r\n","                'state': state,\r\n","                'action': action,\r\n","                'reward': reward\r\n","                }\r\n","            )\r\n","            env.render()\r\n","            epochs += 1\r\n","            total_reward += reward\r\n","\r\n","        total_epochs += epochs\r\n","\r\n","        if i % log_interval == 0:\r\n","            clear_output(wait=True)\r\n","            print(f\"Episode: {i}, n_epochs : {epochs}, reward {total_reward}\")\r\n","\r\n","    print(f\"Results after {n_episodes} episodes:\")\r\n","    print(f\"Average timesteps per episode: {total_epochs / n_episodes}\")\r\n","\r\n","    return frames_RL"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"814lDgBKlWak"},"source":["class Linear(nn.Module):\r\n","    \"\"\"costomized linear layer\"\"\"\r\n","    def __init__(self, in_features, out_features, bias = True, activation_function = None):\r\n","        super(Linear, self).__init__()\r\n","        self.linear = nn.Linear(in_features, out_features, bias = bias)\r\n","        self.activation_function = activation_function if activation_function else lambda x : x\r\n","      \r\n","    def forward(self, x):\r\n","        return self.activation_function(self.linear(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rK3z-wYum9cN"},"source":["class MLP(nn.Module):\r\n","    \"\"\"Multi-layer perceptron\"\"\"\r\n","    def __init__(self, in_features, hidden_features, hidden_layers, out_features, \r\n","                        activation_function = None, init_weights = True, params_seed = 0):\r\n","        torch.manual_seed(params_seed)\r\n","        torch.backends.cudnn.deterministic = True\r\n","        torch.backends.cudnn.benchmark = False\r\n","        super(MLP, self).__init__()\r\n","        net = []\r\n","        net.append(Linear(in_features, hidden_features, True, activation_function))  \r\n","        net += [Linear(hidden_features, hidden_features, True, activation_function) for _ in range(hidden_layers)]  \r\n","        net.append(Linear(hidden_features, out_features, True, None))\r\n","        self.net = nn.Sequential(*net)\r\n","\r\n","        if init_weights :\r\n","            # init_weights : Motivated by \"Implicit Neural Representations with Periodic Activation Functions\" (https://arxiv.org/abs/2006.09661).\r\n","            hidden_omega_0 = 1.0\r\n","            with torch.no_grad():\r\n","                self.net[0].linear.weight.uniform_(-1 / in_features, 1 / in_features)      \r\n","                    \r\n","                for l in range(1, len(self.net)) :\r\n","                    self.net[l].linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \r\n","                                                        np.sqrt(6 / hidden_features) / hidden_omega_0)\r\n","\r\n","    def forward(self, x):\r\n","        return self.net(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dxVW7s8_l2ts"},"source":["def get_Q(input_dim = 2, output_dim = 1, hidden_dim = 20, n_hidden = 1, g = torch.relu, lr = 0.0001):\r\n","    \"\"\"\r\n","    input_dim = env.observation_space.n\r\n","    output_dim = env.action_space.n\r\n","    hidden_dim : dimension des couches cachées\r\n","    n_hidden : nombre de couches cachées\r\n","    g : fonction d'activation (torch.relu, F.softplus, ...)\r\n","    lr : pas d'apprentissage\r\n","    \"\"\"\r\n","    Q = MLP(in_features = input_dim, hidden_features = hidden_dim, hidden_layers = n_hidden, \r\n","            out_features = output_dim, activation_function = g, init_weights = True, params_seed = 0)\r\n","\r\n","    Q.criterion = torch.nn.MSELoss()\r\n","    Q.optimizer = torch.optim.Adam(Q.parameters(), lr = lr)\r\n","\r\n","    return Q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"12NnDJISpsOX"},"source":["get_Q()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Z-3jMTHrYEv"},"source":["# **1) Addition**"]},{"cell_type":"code","metadata":{"id":"jjmoFAW2rjPd"},"source":["low = 1\r\n","high = 100000\r\n","n = 10000\r\n","n_choices = 3\r\n","qcms = get_dataset(low = low, high = high, n = n, n_choices = n_choices)\r\n","train_n_episodes = 1000\r\n","val_n_episodes = 100\r\n","batch_size = n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C94oxgvv5Z8Q"},"source":["## **Résoudre l'environnement sans apprentissage par renforcement**"]},{"cell_type":"code","metadata":{"id":"vC1C4bU-RfIP"},"source":["env = Student(qcms, low, high, n_choices = n_choices)\r\n","env.prepare_dataset(batch_size = batch_size) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2qwSCXS-k108"},"source":["frames_RL = evaluate(env, n_episodes = val_n_episodes)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_1vlPld16Li5"},"source":["## **Entrez dans l'apprentissage par renforcement**"]},{"cell_type":"code","metadata":{"id":"dc_gfwX1Trhb"},"source":["env = Student(qcms, low, high, n_choices = n_choices)\r\n","env.prepare_dataset(batch_size = batch_size) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fxs9QySOaVRL"},"source":["Q = get_Q()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCVqqUylUZsQ"},"source":["Q, all_loss, all_reward = train(env, 1000, Q)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2udcEljBl5wS"},"source":["x = torch.Tensor([1, 70.0])\r\n","Q(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8Wuv9xtUryA"},"source":["frames_RL = evaluate(env, val_n_episodes, Q)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zn0AkmMrTtv7"},"source":["\"\"\"\r\n","# The algorithms require a vectorized environment to run\r\n","env = DummyVecEnv([lambda: Student(qcms, low, high)])\r\n","model = PPO2(MlpPolicy, env, verbose=1)\r\n","\r\n","model.learn(total_timesteps = 2)\r\n","\r\n","obs = env.reset()\r\n","for i in range(2000):\r\n","  action, _states = model.predict(obs)\r\n","  obs, rewards, done, info = env.step(action)\r\n","  env.render()\r\n","\"\"\""],"execution_count":null,"outputs":[]}]}